import{_ as l}from"./index-CStG9y2k.js";let i=null;const u="Llama-3.1-8B-Instruct-q4f32_1-MLC";async function p(t=u){if(!i){const{CreateMLCEngine:n}=await l(async()=>{const{CreateMLCEngine:e}=await import("./index-OfwQNRNX.js");return{CreateMLCEngine:e}},[]);i=n(t,{initProgressCallback:e=>{console.debug("WebLLM init:",e.text)}})}return i}function m(t,n){var s;const e=[];return e.push(`You are ${t.name}, an AI companion with a ${t.personality} personality.`),t.description&&e.push(t.description),(s=t.traits)!=null&&s.length&&e.push(`Traits: ${t.traits.join(", ")}.`),e.push("Respond concisely in 1â€“2 sentences, natural and conversational, and stay in character."),e.push("Avoid explicit content, slurs, or harmful advice. Be supportive and emotionally intelligent."),n?e.push(`Reply in ${n}. If the user's message uses a different language/script, prefer their language.`):e.push("Reply in the language the user used; if unclear, default to English."),e.join(` 
`)}async function y({agent:t,userText:n,language:e}){var o,r,a;const s=await p(),c=[{role:"system",content:m(t,e)},{role:"user",content:n}];return(((a=(r=(o=(await s.chat.completions.create({messages:c,temperature:.8,top_p:.9})).choices)==null?void 0:o[0])==null?void 0:r.message)==null?void 0:a.content)||"").trim()}export{y as generateWithWebLLM,p as getEngine};
